import time
import random
import re
from time import sleep
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def get_product_name(product_name):
    """L√†m s·∫°ch t√™n s·∫£n ph·∫©m, b·ªè ph·∫ßn trong ngo·∫∑c."""
    return re.sub(r"\(.*\)", "", product_name).strip()

def crawl_page(url):
    options = webdriver.ChromeOptions()
    options.headless = False  
    driver = webdriver.Chrome(options=options)

    result = {
        "Product Name": None,
        "Price": None,
        "Specifications": {}
    }

    try:
        driver.get(url)
        sleep(3)  

        # L·∫•y t√™n s·∫£n ph·∫©m v√† l√†m s·∫°ch t√™n
        try:
            product_name = driver.find_element(By.CSS_SELECTOR, "h1.product-name").text
            result["Product Name"] = get_product_name(product_name)
        except Exception as e:
            print(f"[!] Kh√¥ng l·∫•y ƒë∆∞·ª£c t√™n s·∫£n ph·∫©m: {e}")

        # L·∫•y gi√° s·∫£n ph·∫©m
        try:
            price = driver.find_element(By.CSS_SELECTOR, "#product-info-price .text-20.red").text.strip()
            result["Price"] = price
        except Exception as e:
            print(f"[!] Kh√¥ng l·∫•y ƒë∆∞·ª£c gi√°: {e}")

        for _ in range(10):
            try:
                button = WebDriverWait(driver, 2).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "a.viewmore.btn.red"))
                )
                button.click()
                sleep(1)
                break
            except:
                driver.execute_script("window.scrollBy(0, window.innerHeight);")
                sleep(0.5)

        # L·∫•y b·∫£ng th√¥ng s·ªë k·ªπ thu·∫≠t
        try:
            table = driver.find_element(By.CSS_SELECTOR, "#full-spec table")
            for row in table.find_elements(By.TAG_NAME, "tr"):
                cols = row.find_elements(By.TAG_NAME, "td")
                if len(cols) == 2:
                    key = cols[0].text.strip()
                    value = cols[1].text.strip()
                    result["Specifications"][key] = value
        except Exception as e:
            print(f"[!] Kh√¥ng th·ªÉ l·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t: {e}")

    finally:
        driver.quit()

    return result

def crawl_all_products(start_page_url, max_pages):
    options = webdriver.ChromeOptions()
    options.headless = False
    driver = webdriver.Chrome(options=options)

    all_products = []
    page = 1

    try:
        while page <= max_pages:
            if page == 1:
                page_url = start_page_url
            else:
                page_url = f"{start_page_url}?page={page}"

            print(f"üîÑ ƒêang c√†o trang {page}: {page_url}")
            driver.get(page_url)

            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".p-container"))
                )
            except:
                print("‚ùå Kh√¥ng c√≥ th√™m s·∫£n ph·∫©m, d·ª´ng l·∫°i.")
                break

            # L·∫•y danh s√°ch c√°c s·∫£n ph·∫©m tr√™n trang
            try:
                product_elements = driver.find_elements(By.CSS_SELECTOR, ".p-container")
                for product_element in product_elements:
                    try:
                        link = product_element.find_element(By.CSS_SELECTOR, ".p-name a").get_attribute("href")
                        product_details = crawl_page(link) 
                        all_products.append(product_details) 
                    except Exception as e:
                        print(f"‚ö†Ô∏è L·ªói khi l·∫•y th√¥ng tin s·∫£n ph·∫©m: {e}")
                        continue

            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi l·∫•y danh s√°ch s·∫£n ph·∫©m: {e}")
                break

            page += 1
            time.sleep(random.uniform(1, 3))

    finally:
        driver.quit()

    return all_products

def save_to_csv(data_list, filename="products.csv"):
    """
    L∆∞u danh s√°ch s·∫£n ph·∫©m v√†o file CSV.
    T√°ch 'Specifications' th√†nh c√°c c·ªôt ri√™ng bi·ªát.
    """
    if not data_list:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u.")
        return

    flat_data_list = []
    for item in data_list:
        flat_item = item.copy()
        
        if "Specifications" in flat_item and isinstance(flat_item["Specifications"], dict):
            for key, value in flat_item["Specifications"].items():
                flat_item[key] = value
            del flat_item["Specifications"]
        
        flat_data_list.append(flat_item)

    # Chuy·ªÉn th√†nh DataFrame
    df = pd.DataFrame(flat_data_list)

    # L∆∞u DataFrame v√†o CSV
    df.to_csv(filename, index=False, encoding="utf-8")

    print(f" ƒê√£ l∆∞u d·ªØ li·ªáu v√†o file '{filename}'.")

if __name__ == "__main__":
    start_page_url = "https://mega.com.vn/may-tinh-xach-tay.html"  # Thay b·∫±ng URL th·∫≠t
    max_pages = 39  # S·ªë l∆∞·ª£ng trang t·ªëi ƒëa mu·ªën c√†o
    all_products = crawl_all_products(start_page_url, max_pages)

    # In th√¥ng tin chi ti·∫øt t·ª´ng s·∫£n ph·∫©m
    for product in all_products:
        print(f"\n=== ƒêang c√†o s·∫£n ph·∫©m: {product['Product Name']} ===")
        print(f"Product Name: {product['Product Name']}")
        print(f"Price: {product['Price']}")
        if product['Specifications']:
            print("Specifications:")
            for key, value in product['Specifications'].items():
                print(f"  - {key}: {value}")
    
    # L∆∞u k·∫øt qu·∫£ v√†o CSV
    save_to_csv(all_products, "../raw_data/mega.csv")
    print(" Ho√†n t·∫•t c√†o d·ªØ li·ªáu.")

